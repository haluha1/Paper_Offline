{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Search-Vehicle.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["aPQk04j8Kmi2","2dFQZX62K_0X","xErZE8aPLKJ_","gJgPsqUfLW-g","GbnHFdcBLdSN","yCP2bELMLkq0","4KIOrU3tLnrN","8koHvOI5LrbQ","FWJfGqdILuzM","hH-JiDf8Lx16","ZpVnjDpcL1OD","bdBIaZwUL4wr","poH9LB_yMZNb","5ext8BWMMFZO","E0gilrqw23Z0","jsTvVS4B2_Ky"],"toc_visible":true,"mount_file_id":"1uhhLTdURnc2u_S2R7_X6NZna1yYZVcLc","authorship_tag":"ABX9TyOXUye0HXA/sYsQHA1/Mb03"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"q_aW0xk5Kbdi"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aPQk04j8Kmi2"},"source":["### Prepare data"]},{"cell_type":"code","metadata":{"id":"L54hhPkoKc0Y"},"source":["%%capture\n","! pip install mat4py\n","! pip install annoy\n","# # VGG-16 pre-trained weights can find on google\n","# # https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwj9pZ7N-evrAhXCBogKHU8ABmUQFjAAegQIAxAB&url=https%3A%2F%2Fgithub.com%2Ffchollet%2Fdeep-learning-models%2Freleases%2Fdownload%2Fv0.1%2Fvgg16_weights_tf_dim_ordering_tf_kernels.h5&usg=AOvVaw3uMul4ntn2BzUI-UtYyVOm\n","!gdown --id \"1aV0nTjUpk35cxAUrJnCUIplnv50v-x6G\" # VGG-16 pre-trained weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgNuN9NTKUZz"},"source":["import mat4py\n","\n","%tensorflow_version 1.x\n","from __future__ import absolute_import, division, print_function\n","import tensorflow as tf\n","from sklearn.metrics import average_precision_score\n","from optparse import OptionParser\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import time, random\n","import pprint\n","import os, sys\n","import pickle, copy\n","import cv2, math\n","\n","from keras import backend as K\n","from keras import initializers, regularizers\n","from keras.engine import Layer, InputSpec\n","from keras.engine.topology import get_source_inputs\n","from keras.models import Model\n","from keras.objectives import categorical_crossentropy\n","from keras.optimizers import Adam, SGD, RMSprop\n","from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n","from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n","from keras.utils import generic_utils, layer_utils\n","from keras.utils.data_utils import get_file\n","\n","from annoy import AnnoyIndex\n","from collections import Counter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xMyzSKApKepl"},"source":["dataset = mat4py.loadmat('BIT-Vehicle/VehicleInfo.mat')\n","df = pd.DataFrame(dataset[\"VehicleInfo\"])\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mUNbin6c983d"},"source":["# We just get single class image only\n","rm_imgs = df[df[\"nVehicles\"]>1].index\n","df=df.drop(rm_imgs)\n","df[\"label\"]=df[\"vehicles\"].str[\"category\"]\n","df[\"label\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6EEAXPvBKg-l"},"source":["from sklearn.model_selection import train_test_split\n","df_train, df_test = train_test_split(df,test_size=0.2,random_state=2020)\n","df_train = df_train.reset_index().drop(columns=[\"index\"])\n","df_test = df_test.reset_index().drop(columns=[\"index\"])\n","print(df_train.shape)\n","print(df_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kMyeXYj-KpOL"},"source":["## Testing"]},{"cell_type":"markdown","metadata":{"id":"2dFQZX62K_0X"},"source":["### Define function"]},{"cell_type":"markdown","metadata":{"id":"xErZE8aPLKJ_"},"source":["#### Config setting"]},{"cell_type":"code","metadata":{"id":"kZa8WUhuKwEF"},"source":["class Config:\n","\n","\tdef __init__(self):\n","\n","\t\t# Print the process or not\n","\t\tself.verbose = True\n","\n","\t\t# Name of base network\n","\t\tself.network = 'vgg'\n","\n","\t\t# Setting for data augmentation\n","\t\tself.use_horizontal_flips = False\n","\t\tself.use_vertical_flips = False\n","\t\tself.rot_90 = False\n","\n","\t\t# Anchor box scales\n","    # Note that if im_size is smaller, anchor_box_scales should be scaled\n","    # Original anchor_box_scales in the paper is [128, 256, 512]\n","\t\t# self.anchor_box_scales = [64, 128, 256] \t\t\n","\t\tself.anchor_box_scales = [128, 256, 512] \n","\n","\t\t# Anchor box ratios\n","\t\tself.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n","\n","\t\t# Size to resize the smallest side of the image\n","\t\t# Original setting in paper is 600. Set to 300 in here to save training time\n","\t\tself.im_size = 300\n","\n","\t\t# image channel-wise mean to subtract\n","\t\tself.img_channel_mean = [103.939, 116.779, 123.68]\n","\t\tself.img_scaling_factor = 1.0\n","\n","\t\t# number of ROIs at once\n","\t\tself.num_rois = 4\n","\n","\t\t# stride at the RPN (this depends on the network configuration)\n","\t\tself.rpn_stride = 16\n","\n","\t\tself.balanced_classes = False\n","\n","\t\t# scaling the stdev\n","\t\tself.std_scaling = 4.0\n","\t\tself.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n","\n","\t\t# overlaps for RPN\n","\t\tself.rpn_min_overlap = 0.3\n","\t\tself.rpn_max_overlap = 0.7\n","\n","\t\t# overlaps for classifier ROIs\n","\t\tself.classifier_min_overlap = 0.1\n","\t\tself.classifier_max_overlap = 0.5\n","\n","\t\t# placeholder for the class mapping, automatically generated by the parser\n","\t\tself.class_mapping = None\n","\n","\t\tself.model_path = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gJgPsqUfLW-g"},"source":["#### Parser the data from annotation file"]},{"cell_type":"code","metadata":{"id":"gNN6hoVILa1z"},"source":["def get_data(input_path):\n","\t\"\"\"Parser the data from annotation file\n","\t\n","\tArgs:\n","\t\tinput_path: annotation file path\n","\n","\tReturns:\n","\t\tall_data: list(filepath, width, height, list(bboxes))\n","\t\tclasses_count: dict{key:class_name, value:count_num} \n","\t\t\te.g. {'Car': 2383, 'Mobile phone': 1108, 'Person': 3745}\n","\t\tclass_mapping: dict{key:class_name, value: idx}\n","\t\t\te.g. {'Car': 0, 'Mobile phone': 1, 'Person': 2}\n","\t\"\"\"\n","\tfound_bg = False\n","\tall_imgs = {}\n","\n","\tclasses_count = {}\n","\n","\tclass_mapping = {}\n","\n","\tvisualise = True\n","\n","\ti = 1\n","\t\n","\twith open(input_path,'r') as f:\n","\n","\t\tprint('Parsing annotation files')\n","\n","\t\tfor line in f:\n","\n","\t\t\t# Print process\n","\t\t\tsys.stdout.write('\\r'+'idx=' + str(i))\n","\t\t\ti += 1\n","\n","\t\t\tline_split = line.strip().split(',')\n","\n","\t\t\t# Make sure the info saved in annotation file matching the format (path_filename, x1, y1, x2, y2, class_name)\n","\t\t\t# Note:\n","\t\t\t#\tOne path_filename might has several classes (class_name)\n","\t\t\t#\tx1, y1, x2, y2 are the pixel value of the origial image, not the ratio value\n","\t\t\t#\t(x1, y1) top left coordinates; (x2, y2) bottom right coordinates\n","\t\t\t#   x1,y1-------------------\n","\t\t\t#\t|\t\t\t\t\t\t|\n","\t\t\t#\t|\t\t\t\t\t\t|\n","\t\t\t#\t|\t\t\t\t\t\t|\n","\t\t\t#\t|\t\t\t\t\t\t|\n","\t\t\t#\t---------------------x2,y2\n","\n","\t\t\t(filename,x1,y1,x2,y2,class_name) = line_split\n","\n","\t\t\tif class_name not in classes_count:\n","\t\t\t\tclasses_count[class_name] = 1\n","\t\t\telse:\n","\t\t\t\tclasses_count[class_name] += 1\n","\n","\t\t\tif class_name not in class_mapping:\n","\t\t\t\tif class_name == 'bg' and found_bg == False:\n","\t\t\t\t\tprint('Found class name with special name bg. Will be treated as a background region (this is usually for hard negative mining).')\n","\t\t\t\t\tfound_bg = True\n","\t\t\t\tclass_mapping[class_name] = len(class_mapping)\n","\n","\t\t\tif filename not in all_imgs:\n","\t\t\t\tall_imgs[filename] = {}\n","\t\t\t\t\n","\t\t\t\timg = cv2.imread(filename)\n","\t\t\t\t(rows,cols) = img.shape[:2]\n","\t\t\t\tall_imgs[filename]['filepath'] = filename\n","\t\t\t\tall_imgs[filename]['width'] = cols\n","\t\t\t\tall_imgs[filename]['height'] = rows\n","\t\t\t\tall_imgs[filename]['bboxes'] = []\n","\t\t\t\t# if np.random.randint(0,6) > 0:\n","\t\t\t\t# \tall_imgs[filename]['imageset'] = 'trainval'\n","\t\t\t\t# else:\n","\t\t\t\t# \tall_imgs[filename]['imageset'] = 'test'\n","\n","\t\t\tall_imgs[filename]['bboxes'].append({'class': class_name, 'x1': int(x1), 'x2': int(x2), 'y1': int(y1), 'y2': int(y2)})\n","\n","\n","\t\tall_data = []\n","\t\tfor key in all_imgs:\n","\t\t\tall_data.append(all_imgs[key])\n","\t\t\n","\t\t# make sure the bg class is last in the list\n","\t\tif found_bg:\n","\t\t\tif class_mapping['bg'] != len(class_mapping) - 1:\n","\t\t\t\tkey_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping)-1][0]\n","\t\t\t\tval_to_switch = class_mapping['bg']\n","\t\t\t\tclass_mapping['bg'] = len(class_mapping) - 1\n","\t\t\t\tclass_mapping[key_to_switch] = val_to_switch\n","\t\t\n","\t\treturn all_data, classes_count, class_mapping"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GbnHFdcBLdSN"},"source":["#### Define ROI Pooling Convolutional Layer"]},{"cell_type":"code","metadata":{"id":"btluTb61LfbR"},"source":["class RoiPoolingConv(Layer):\n","    '''ROI pooling layer for 2D inputs.\n","    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n","    K. He, X. Zhang, S. Ren, J. Sun\n","    # Arguments\n","        pool_size: int\n","            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n","        num_rois: number of regions of interest to be used\n","    # Input shape\n","        list of two 4D tensors [X_img,X_roi] with shape:\n","        X_img:\n","        `(1, rows, cols, channels)`\n","        X_roi:\n","        `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n","    # Output shape\n","        3D tensor with shape:\n","        `(1, num_rois, channels, pool_size, pool_size)`\n","    '''\n","    def __init__(self, pool_size, num_rois, **kwargs):\n","\n","        self.dim_ordering = K.common.image_dim_ordering()\n","        self.pool_size = pool_size\n","        self.num_rois = num_rois\n","\n","        super(RoiPoolingConv, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.nb_channels = input_shape[0][3]   \n","\n","    def compute_output_shape(self, input_shape):\n","        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n","\n","    def call(self, x, mask=None):\n","\n","        assert(len(x) == 2)\n","\n","        # x[0] is image with shape (rows, cols, channels)\n","        img = x[0]\n","\n","        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n","        rois = x[1]\n","\n","        input_shape = K.shape(img)\n","\n","        outputs = []\n","\n","        for roi_idx in range(self.num_rois):\n","\n","            x = rois[0, roi_idx, 0]\n","            y = rois[0, roi_idx, 1]\n","            w = rois[0, roi_idx, 2]\n","            h = rois[0, roi_idx, 3]\n","\n","            x = K.cast(x, 'int32')\n","            y = K.cast(y, 'int32')\n","            w = K.cast(w, 'int32')\n","            h = K.cast(h, 'int32')\n","\n","            # Resized roi of the image to pooling size (7x7)\n","            rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n","            outputs.append(rs)\n","                \n","\n","        final_output = K.concatenate(outputs, axis=0)\n","\n","        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n","        # Might be (1, 4, 7, 7, 3)\n","        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n","\n","        # permute_dimensions is similar to transpose\n","        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n","\n","        return final_output\n","    \n","    \n","    def get_config(self):\n","        config = {'pool_size': self.pool_size,\n","                  'num_rois': self.num_rois}\n","        base_config = super(RoiPoolingConv, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yCP2bELMLkq0"},"source":["#### Vgg-16 model"]},{"cell_type":"code","metadata":{"id":"C2pW2LnrLjxn"},"source":["def get_img_output_length(width, height):\n","    def get_output_length(input_length):\n","        return input_length//16\n","\n","    return get_output_length(width), get_output_length(height)    \n","\n","def nn_base(input_tensor=None, trainable=False):\n","\n","\n","    input_shape = (None, None, 3)\n","\n","    if input_tensor is None:\n","        img_input = Input(shape=input_shape)\n","    else:\n","        if not K.is_keras_tensor(input_tensor):\n","            img_input = Input(tensor=input_tensor, shape=input_shape)\n","        else:\n","            img_input = input_tensor\n","\n","    bn_axis = 3\n","\n","    # Block 1\n","    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n","    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n","\n","    # Block 2\n","    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n","    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n","\n","    # Block 3\n","    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n","    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n","    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n","\n","    # Block 4\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n","\n","    # Block 5\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n","    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n","\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4KIOrU3tLnrN"},"source":["####  RPN layer"]},{"cell_type":"code","metadata":{"id":"yMi3bK-dLo2w"},"source":["def rpn_layer(base_layers, num_anchors):\n","    \"\"\"Create a rpn layer\n","        Step1: Pass through the feature map from base layer to a 3x3 512 channels convolutional layer\n","                Keep the padding 'same' to preserve the feature map's size\n","        Step2: Pass the step1 to two (1,1) convolutional layer to replace the fully connected layer\n","                classification layer: num_anchors (9 in here) channels for 0, 1 sigmoid activation output\n","                regression layer: num_anchors*4 (36 in here) channels for computing the regression of bboxes with linear activation\n","    Args:\n","        base_layers: vgg in here\n","        num_anchors: 9 in here\n","\n","    Returns:\n","        [x_class, x_regr, base_layers]\n","        x_class: classification for whether it's an object\n","        x_regr: bboxes regression\n","        base_layers: vgg in here\n","    \"\"\"\n","    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n","\n","    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n","    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n","\n","    return [x_class, x_regr, base_layers]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8koHvOI5LrbQ"},"source":["####  Classifier layer"]},{"cell_type":"code","metadata":{"id":"R94loSeLLtAs"},"source":["def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 4):\n","    \"\"\"Create a classifier layer\n","    \n","    Args:\n","        base_layers: vgg\n","        input_rois: `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n","        num_rois: number of rois to be processed in one time (4 in here)\n","\n","    Returns:\n","        list(out_class, out_regr)\n","        out_class: classifier layer output\n","        out_regr: regression layer output\n","    \"\"\"\n","\n","    input_shape = (num_rois,7,7,512)\n","\n","    pooling_regions = 7\n","\n","    # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)\n","    # num_rois (4) 7x7 roi pooling\n","    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n","\n","    # Flatten the convlutional layer and connected to 2 FC and 2 dropout\n","    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\n","    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)\n","    out = TimeDistributed(Dropout(0.5))(out)\n","    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)\n","    out = TimeDistributed(Dropout(0.5))(out)\n","\n","    # There are two output layer\n","    # out_class: softmax acivation function for classify the class name of the object\n","    # out_regr: linear activation function for bboxes coordinates regression\n","    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n","    # note: no regression target for bg class\n","    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n","\n","    return [out_class, out_regr]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FWJfGqdILuzM"},"source":["#### Calculate IoU (Intersection of Union)"]},{"cell_type":"code","metadata":{"id":"rkudjL0tLv8N"},"source":["def union(au, bu, area_intersection):\n","\tarea_a = (au[2] - au[0]) * (au[3] - au[1])\n","\tarea_b = (bu[2] - bu[0]) * (bu[3] - bu[1])\n","\tarea_union = area_a + area_b - area_intersection\n","\treturn area_union\n","\n","\n","def intersection(ai, bi):\n","\tx = max(ai[0], bi[0])\n","\ty = max(ai[1], bi[1])\n","\tw = min(ai[2], bi[2]) - x\n","\th = min(ai[3], bi[3]) - y\n","\tif w < 0 or h < 0:\n","\t\treturn 0\n","\treturn w*h\n","\n","\n","def iou(a, b):\n","\t# a and b should be (x1,y1,x2,y2)\n","\n","\tif a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n","\t\treturn 0.0\n","\n","\tarea_i = intersection(a, b)\n","\tarea_u = union(a, b, area_i)\n","\n","\treturn float(area_i) / float(area_u + 1e-6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hH-JiDf8Lx16"},"source":["#### Calculate the rpn for all anchors of all images"]},{"cell_type":"code","metadata":{"id":"lO3zPm9tLzCE"},"source":["def calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n","\t\"\"\"(Important part!) Calculate the rpn for all anchors \n","\t\tIf feature map has shape 38x50=1900, there are 1900x9=17100 potential anchors\n","\t\n","\tArgs:\n","\t\tC: config\n","\t\timg_data: augmented image data\n","\t\twidth: original image width (e.g. 600)\n","\t\theight: original image height (e.g. 800)\n","\t\tresized_width: resized image width according to C.im_size (e.g. 300)\n","\t\tresized_height: resized image height according to C.im_size (e.g. 400)\n","\t\timg_length_calc_function: function to calculate final layer's feature map (of base model) size according to input image size\n","\n","\tReturns:\n","\t\ty_rpn_cls: list(num_bboxes, y_is_box_valid + y_rpn_overlap)\n","\t\t\ty_is_box_valid: 0 or 1 (0 means the box is invalid, 1 means the box is valid)\n","\t\t\ty_rpn_overlap: 0 or 1 (0 means the box is not an object, 1 means the box is an object)\n","\t\ty_rpn_regr: list(num_bboxes, 4*y_rpn_overlap + y_rpn_regr)\n","\t\t\ty_rpn_regr: x1,y1,x2,y2 bunding boxes coordinates\n","\t\"\"\"\n","\tdownscale = float(C.rpn_stride) \n","\tanchor_sizes = C.anchor_box_scales   # 128, 256, 512\n","\tanchor_ratios = C.anchor_box_ratios  # 1:1, 1:2*sqrt(2), 2*sqrt(2):1\n","\tnum_anchors = len(anchor_sizes) * len(anchor_ratios) # 3x3=9\n","\n","\t# calculate the output map size based on the network architecture\n","\t(output_width, output_height) = img_length_calc_function(resized_width, resized_height)\n","\n","\tn_anchratios = len(anchor_ratios)    # 3\n","\t\n","\t# initialise empty output objectives\n","\ty_rpn_overlap = np.zeros((output_height, output_width, num_anchors))\n","\ty_is_box_valid = np.zeros((output_height, output_width, num_anchors))\n","\ty_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))\n","\n","\tnum_bboxes = len(img_data['bboxes'])\n","\n","\tnum_anchors_for_bbox = np.zeros(num_bboxes).astype(int)\n","\tbest_anchor_for_bbox = -1*np.ones((num_bboxes, 4)).astype(int)\n","\tbest_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)\n","\tbest_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)\n","\tbest_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)\n","\n","\t# get the GT box coordinates, and resize to account for image resizing\n","\tgta = np.zeros((num_bboxes, 4))\n","\tfor bbox_num, bbox in enumerate(img_data['bboxes']):\n","\t\t# get the GT box coordinates, and resize to account for image resizing\n","\t\tgta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n","\t\tgta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n","\t\tgta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n","\t\tgta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n","\t\n","\t# rpn ground truth\n","\n","\tfor anchor_size_idx in range(len(anchor_sizes)):\n","\t\tfor anchor_ratio_idx in range(n_anchratios):\n","\t\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n","\t\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\t\n","\t\t\t\n","\t\t\tfor ix in range(output_width):\t\t\t\t\t\n","\t\t\t\t# x-coordinates of the current anchor box\t\n","\t\t\t\tx1_anc = downscale * (ix + 0.5) - anchor_x / 2\n","\t\t\t\tx2_anc = downscale * (ix + 0.5) + anchor_x / 2\t\n","\t\t\t\t\n","\t\t\t\t# ignore boxes that go across image boundaries\t\t\t\t\t\n","\t\t\t\tif x1_anc < 0 or x2_anc > resized_width:\n","\t\t\t\t\tcontinue\n","\t\t\t\t\t\n","\t\t\t\tfor jy in range(output_height):\n","\n","\t\t\t\t\t# y-coordinates of the current anchor box\n","\t\t\t\t\ty1_anc = downscale * (jy + 0.5) - anchor_y / 2\n","\t\t\t\t\ty2_anc = downscale * (jy + 0.5) + anchor_y / 2\n","\n","\t\t\t\t\t# ignore boxes that go across image boundaries\n","\t\t\t\t\tif y1_anc < 0 or y2_anc > resized_height:\n","\t\t\t\t\t\tcontinue\n","\n","\t\t\t\t\t# bbox_type indicates whether an anchor should be a target\n","\t\t\t\t\t# Initialize with 'negative'\n","\t\t\t\t\tbbox_type = 'neg'\n","\n","\t\t\t\t\t# this is the best IOU for the (x,y) coord and the current anchor\n","\t\t\t\t\t# note that this is different from the best IOU for a GT bbox\n","\t\t\t\t\tbest_iou_for_loc = 0.0\n","\n","\t\t\t\t\tfor bbox_num in range(num_bboxes):\n","\t\t\t\t\t\t\n","\t\t\t\t\t\t# get IOU of the current GT box and the current anchor box\n","\t\t\t\t\t\tcurr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1_anc, y1_anc, x2_anc, y2_anc])\n","\t\t\t\t\t\t# calculate the regression targets if they will be needed\n","\t\t\t\t\t\tif curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:\n","\t\t\t\t\t\t\tcx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n","\t\t\t\t\t\t\tcy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n","\t\t\t\t\t\t\tcxa = (x1_anc + x2_anc)/2.0\n","\t\t\t\t\t\t\tcya = (y1_anc + y2_anc)/2.0\n","\n","\t\t\t\t\t\t\t# x,y are the center point of ground-truth bbox\n","\t\t\t\t\t\t\t# xa,ya are the center point of anchor bbox (xa=downscale * (ix + 0.5); ya=downscale * (iy+0.5))\n","\t\t\t\t\t\t\t# w,h are the width and height of ground-truth bbox\n","\t\t\t\t\t\t\t# wa,ha are the width and height of anchor bboxe\n","\t\t\t\t\t\t\t# tx = (x - xa) / wa\n","\t\t\t\t\t\t\t# ty = (y - ya) / ha\n","\t\t\t\t\t\t\t# tw = log(w / wa)\n","\t\t\t\t\t\t\t# th = log(h / ha)\n","\t\t\t\t\t\t\ttx = (cx - cxa) / (x2_anc - x1_anc)\n","\t\t\t\t\t\t\tty = (cy - cya) / (y2_anc - y1_anc)\n","\t\t\t\t\t\t\ttw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n","\t\t\t\t\t\t\tth = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))\n","\t\t\t\t\t\t\n","\t\t\t\t\t\tif img_data['bboxes'][bbox_num]['class'] != 'bg':\n","\n","\t\t\t\t\t\t\t# all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best\n","\t\t\t\t\t\t\tif curr_iou > best_iou_for_bbox[bbox_num]:\n","\t\t\t\t\t\t\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n","\t\t\t\t\t\t\t\tbest_iou_for_bbox[bbox_num] = curr_iou\n","\t\t\t\t\t\t\t\tbest_x_for_bbox[bbox_num,:] = [x1_anc, x2_anc, y1_anc, y2_anc]\n","\t\t\t\t\t\t\t\tbest_dx_for_bbox[bbox_num,:] = [tx, ty, tw, th]\n","\n","\t\t\t\t\t\t\t# we set the anchor to positive if the IOU is >0.7 (it does not matter if there was another better box, it just indicates overlap)\n","\t\t\t\t\t\t\tif curr_iou > C.rpn_max_overlap:\n","\t\t\t\t\t\t\t\tbbox_type = 'pos'\n","\t\t\t\t\t\t\t\tnum_anchors_for_bbox[bbox_num] += 1\n","\t\t\t\t\t\t\t\t# we update the regression layer target if this IOU is the best for the current (x,y) and anchor position\n","\t\t\t\t\t\t\t\tif curr_iou > best_iou_for_loc:\n","\t\t\t\t\t\t\t\t\tbest_iou_for_loc = curr_iou\n","\t\t\t\t\t\t\t\t\tbest_regr = (tx, ty, tw, th)\n","\n","\t\t\t\t\t\t\t# if the IOU is >0.3 and <0.7, it is ambiguous and no included in the objective\n","\t\t\t\t\t\t\tif C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:\n","\t\t\t\t\t\t\t\t# gray zone between neg and pos\n","\t\t\t\t\t\t\t\tif bbox_type != 'pos':\n","\t\t\t\t\t\t\t\t\tbbox_type = 'neutral'\n","\n","\t\t\t\t\t# turn on or off outputs depending on IOUs\n","\t\t\t\t\tif bbox_type == 'neg':\n","\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n","\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n","\t\t\t\t\telif bbox_type == 'neutral':\n","\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n","\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n","\t\t\t\t\telif bbox_type == 'pos':\n","\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n","\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n","\t\t\t\t\t\tstart = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n","\t\t\t\t\t\ty_rpn_regr[jy, ix, start:start+4] = best_regr\n","\n","\t# we ensure that every bbox has at least one positive RPN region\n","\n","\tfor idx in range(num_anchors_for_bbox.shape[0]):\n","\t\tif num_anchors_for_bbox[idx] == 0:\n","\t\t\t# no box with an IOU greater than zero ...\n","\t\t\tif best_anchor_for_bbox[idx, 0] == -1:\n","\t\t\t\tcontinue\n","\t\t\ty_is_box_valid[\n","\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n","\t\t\t\tbest_anchor_for_bbox[idx,3]] = 1\n","\t\t\ty_rpn_overlap[\n","\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n","\t\t\t\tbest_anchor_for_bbox[idx,3]] = 1\n","\t\t\tstart = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])\n","\t\t\ty_rpn_regr[\n","\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n","\n","\ty_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n","\ty_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n","\n","\ty_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n","\ty_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n","\n","\ty_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n","\ty_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n","\n","\tpos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))\n","\tneg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\n","\n","\tnum_pos = len(pos_locs[0])\n","\n","\t# one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative\n","\t# regions. We also limit it to 256 regions.\n","\tnum_regions = 256\n","\n","\tif len(pos_locs[0]) > num_regions/2:\n","\t\tval_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions/2)\n","\t\ty_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n","\t\tnum_pos = num_regions/2\n","\n","\tif len(neg_locs[0]) + num_pos > num_regions:\n","\t\tval_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n","\t\ty_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n","\n","\ty_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)\n","\ty_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)\n","\n","\treturn np.copy(y_rpn_cls), np.copy(y_rpn_regr), num_pos"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZpVnjDpcL1OD"},"source":["#### Get new image size and augment the image"]},{"cell_type":"code","metadata":{"id":"txSfbSoiL2Z-"},"source":["def get_new_img_size(width, height, img_min_side=300):\n","\tif width <= height:\n","\t\tf = float(img_min_side) / width\n","\t\tresized_height = int(f * height)\n","\t\tresized_width = img_min_side\n","\telse:\n","\t\tf = float(img_min_side) / height\n","\t\tresized_width = int(f * width)\n","\t\tresized_height = img_min_side\n","\n","\treturn resized_width, resized_height\n","\n","def augment(img_data, config, augment=True):\n","\tassert 'filepath' in img_data\n","\tassert 'bboxes' in img_data\n","\tassert 'width' in img_data\n","\tassert 'height' in img_data\n","\n","\timg_data_aug = copy.deepcopy(img_data)\n","\n","\timg = cv2.imread(img_data_aug['filepath'])\n","\n","\tif augment:\n","\t\trows, cols = img.shape[:2]\n","\n","\t\tif config.use_horizontal_flips and np.random.randint(0, 2) == 0:\n","\t\t\timg = cv2.flip(img, 1)\n","\t\t\tfor bbox in img_data_aug['bboxes']:\n","\t\t\t\tx1 = bbox['x1']\n","\t\t\t\tx2 = bbox['x2']\n","\t\t\t\tbbox['x2'] = cols - x1\n","\t\t\t\tbbox['x1'] = cols - x2\n","\n","\t\tif config.use_vertical_flips and np.random.randint(0, 2) == 0:\n","\t\t\timg = cv2.flip(img, 0)\n","\t\t\tfor bbox in img_data_aug['bboxes']:\n","\t\t\t\ty1 = bbox['y1']\n","\t\t\t\ty2 = bbox['y2']\n","\t\t\t\tbbox['y2'] = rows - y1\n","\t\t\t\tbbox['y1'] = rows - y2\n","\n","\t\tif config.rot_90:\n","\t\t\tangle = np.random.choice([0,90,180,270],1)[0]\n","\t\t\tif angle == 270:\n","\t\t\t\timg = np.transpose(img, (1,0,2))\n","\t\t\t\timg = cv2.flip(img, 0)\n","\t\t\telif angle == 180:\n","\t\t\t\timg = cv2.flip(img, -1)\n","\t\t\telif angle == 90:\n","\t\t\t\timg = np.transpose(img, (1,0,2))\n","\t\t\t\timg = cv2.flip(img, 1)\n","\t\t\telif angle == 0:\n","\t\t\t\tpass\n","\n","\t\t\tfor bbox in img_data_aug['bboxes']:\n","\t\t\t\tx1 = bbox['x1']\n","\t\t\t\tx2 = bbox['x2']\n","\t\t\t\ty1 = bbox['y1']\n","\t\t\t\ty2 = bbox['y2']\n","\t\t\t\tif angle == 270:\n","\t\t\t\t\tbbox['x1'] = y1\n","\t\t\t\t\tbbox['x2'] = y2\n","\t\t\t\t\tbbox['y1'] = cols - x2\n","\t\t\t\t\tbbox['y2'] = cols - x1\n","\t\t\t\telif angle == 180:\n","\t\t\t\t\tbbox['x2'] = cols - x1\n","\t\t\t\t\tbbox['x1'] = cols - x2\n","\t\t\t\t\tbbox['y2'] = rows - y1\n","\t\t\t\t\tbbox['y1'] = rows - y2\n","\t\t\t\telif angle == 90:\n","\t\t\t\t\tbbox['x1'] = rows - y2\n","\t\t\t\t\tbbox['x2'] = rows - y1\n","\t\t\t\t\tbbox['y1'] = x1\n","\t\t\t\t\tbbox['y2'] = x2        \n","\t\t\t\telif angle == 0:\n","\t\t\t\t\tpass\n","\n","\timg_data_aug['width'] = img.shape[1]\n","\timg_data_aug['height'] = img.shape[0]\n","\treturn img_data_aug, img"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bdBIaZwUL4wr"},"source":["#### Generate the ground_truth anchors"]},{"cell_type":"code","metadata":{"id":"j8BJIL2OL56G"},"source":["def get_anchor_gt(all_img_data, C, img_length_calc_function, mode='train'):\n","\t\"\"\" Yield the ground-truth anchors as Y (labels)\n","\t\t\n","\tArgs:\n","\t\tall_img_data: list(filepath, width, height, list(bboxes))\n","\t\tC: config\n","\t\timg_length_calc_function: function to calculate final layer's feature map (of base model) size according to input image size\n","\t\tmode: 'train' or 'test'; 'train' mode need augmentation\n","\n","\tReturns:\n","\t\tx_img: image data after resized and scaling (smallest size = 300px)\n","\t\tY: [y_rpn_cls, y_rpn_regr]\n","\t\timg_data_aug: augmented image data (original image with augmentation)\n","\t\tdebug_img: show image for debug\n","\t\tnum_pos: show number of positive anchors for debug\n","\t\"\"\"\n","\twhile True:\n","\n","\t\tfor img_data in all_img_data:\n","\t\t\ttry:\n","\n","\t\t\t\t# read in image, and optionally add augmentation\n","\n","\t\t\t\tif mode == 'train':\n","\t\t\t\t\timg_data_aug, x_img = augment(img_data, C, augment=True)\n","\t\t\t\telse:\n","\t\t\t\t\timg_data_aug, x_img = augment(img_data, C, augment=False)\n","\n","\t\t\t\t(width, height) = (img_data_aug['width'], img_data_aug['height'])\n","\t\t\t\t(rows, cols, _) = x_img.shape\n","\n","\t\t\t\tassert cols == width\n","\t\t\t\tassert rows == height\n","\n","\t\t\t\t# get image dimensions for resizing\n","\t\t\t\t(resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n","\n","\t\t\t\t# resize the image so that smalles side is length = 300px\n","\t\t\t\tx_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)\n","\t\t\t\tdebug_img = x_img.copy()\n","\n","\t\t\t\ttry:\n","\t\t\t\t\ty_rpn_cls, y_rpn_regr, num_pos = calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)\n","\t\t\t\texcept:\n","\t\t\t\t\tcontinue\n","\n","\t\t\t\t# Zero-center by mean pixel, and preprocess image\n","\n","\t\t\t\tx_img = x_img[:,:, (2, 1, 0)]  # BGR -> RGB\n","\t\t\t\tx_img = x_img.astype(np.float32)\n","\t\t\t\tx_img[:, :, 0] -= C.img_channel_mean[0]\n","\t\t\t\tx_img[:, :, 1] -= C.img_channel_mean[1]\n","\t\t\t\tx_img[:, :, 2] -= C.img_channel_mean[2]\n","\t\t\t\tx_img /= C.img_scaling_factor\n","\n","\t\t\t\tx_img = np.transpose(x_img, (2, 0, 1))\n","\t\t\t\tx_img = np.expand_dims(x_img, axis=0)\n","\n","\t\t\t\ty_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= C.std_scaling\n","\n","\t\t\t\tx_img = np.transpose(x_img, (0, 2, 3, 1))\n","\t\t\t\ty_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))\n","\t\t\t\ty_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))\n","\n","\t\t\t\tyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug, debug_img, num_pos\n","\n","\t\t\texcept Exception as e:\n","\t\t\t\tprint(e)\n","\t\t\t\tcontinue"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wsl9LFZTL7p2"},"source":["def non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n","\t# code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n","\t# if there are no boxes, return an empty list\n","  \n","    # Process explanation:\n","    #   Step 1: Sort the probs list\n","    #   Step 2: Find the larget prob 'Last' in the list and save it to the pick list\n","    #   Step 3: Calculate the IoU with 'Last' box and other boxes in the list. If the IoU is larger than overlap_threshold, delete the box from list\n","    #   Step 4: Repeat step 2 and step 3 until there is no item in the probs list \n","\tif len(boxes) == 0:\n","\t\treturn []\n","\n","\t# grab the coordinates of the bounding boxes\n","\tx1 = boxes[:, 0]\n","\ty1 = boxes[:, 1]\n","\tx2 = boxes[:, 2]\n","\ty2 = boxes[:, 3]\n","\n","\tnp.testing.assert_array_less(x1, x2)\n","\tnp.testing.assert_array_less(y1, y2)\n","\n","\t# if the bounding boxes integers, convert them to floats --\n","\t# this is important since we'll be doing a bunch of divisions\n","\tif boxes.dtype.kind == \"i\":\n","\t\tboxes = boxes.astype(\"float\")\n","\n","\t# initialize the list of picked indexes\t\n","\tpick = []\n","\n","\t# calculate the areas\n","\tarea = (x2 - x1) * (y2 - y1)\n","\n","\t# sort the bounding boxes \n","\tidxs = np.argsort(probs)\n","\n","\t# keep looping while some indexes still remain in the indexes\n","\t# list\n","\twhile len(idxs) > 0:\n","\t\t# grab the last index in the indexes list and add the\n","\t\t# index value to the list of picked indexes\n","\t\tlast = len(idxs) - 1\n","\t\ti = idxs[last]\n","\t\tpick.append(i)\n","\n","\t\t# find the intersection\n","\n","\t\txx1_int = np.maximum(x1[i], x1[idxs[:last]])\n","\t\tyy1_int = np.maximum(y1[i], y1[idxs[:last]])\n","\t\txx2_int = np.minimum(x2[i], x2[idxs[:last]])\n","\t\tyy2_int = np.minimum(y2[i], y2[idxs[:last]])\n","\n","\t\tww_int = np.maximum(0, xx2_int - xx1_int)\n","\t\thh_int = np.maximum(0, yy2_int - yy1_int)\n","\n","\t\tarea_int = ww_int * hh_int\n","\n","\t\t# find the union\n","\t\tarea_union = area[i] + area[idxs[:last]] - area_int\n","\n","\t\t# compute the ratio of overlap\n","\t\toverlap = area_int/(area_union + 1e-6)\n","\n","\t\t# delete all indexes from the index list that have\n","\t\tidxs = np.delete(idxs, np.concatenate(([last],\n","\t\t\tnp.where(overlap > overlap_thresh)[0])))\n","\n","\t\tif len(pick) >= max_boxes:\n","\t\t\tbreak\n","\n","\t# return only the bounding boxes that were picked using the integer data type\n","\tboxes = boxes[pick].astype(\"int\")\n","\tprobs = probs[pick]\n","\treturn boxes, probs\n","\n","def apply_regr_np(X, T):\n","\t\"\"\"Apply regression layer to all anchors in one feature map\n","\n","\tArgs:\n","\t\tX: shape=(4, 18, 25) the current anchor type for all points in the feature map\n","\t\tT: regression layer shape=(4, 18, 25)\n","\n","\tReturns:\n","\t\tX: regressed position and size for current anchor\n","\t\"\"\"\n","\ttry:\n","\t\tx = X[0, :, :]\n","\t\ty = X[1, :, :]\n","\t\tw = X[2, :, :]\n","\t\th = X[3, :, :]\n","\n","\t\ttx = T[0, :, :]\n","\t\tty = T[1, :, :]\n","\t\ttw = T[2, :, :]\n","\t\tth = T[3, :, :]\n","\n","\t\tcx = x + w/2.\n","\t\tcy = y + h/2.\n","\t\tcx1 = tx * w + cx\n","\t\tcy1 = ty * h + cy\n","\n","\t\tw1 = np.exp(tw.astype(np.float64)) * w\n","\t\th1 = np.exp(th.astype(np.float64)) * h\n","\t\tx1 = cx1 - w1/2.\n","\t\ty1 = cy1 - h1/2.\n","\n","\t\tx1 = np.round(x1)\n","\t\ty1 = np.round(y1)\n","\t\tw1 = np.round(w1)\n","\t\th1 = np.round(h1)\n","\t\treturn np.stack([x1, y1, w1, h1])\n","\texcept Exception as e:\n","\t\tprint(e)\n","\t\treturn X\n","    \n","def apply_regr(x, y, w, h, tx, ty, tw, th):\n","    # Apply regression to x, y, w and h\n","\ttry:\n","\t\tcx = x + w/2.\n","\t\tcy = y + h/2.\n","\t\tcx1 = tx * w + cx\n","\t\tcy1 = ty * h + cy\n","\t\tw1 = math.exp(tw) * w\n","\t\th1 = math.exp(th) * h\n","\t\tx1 = cx1 - w1/2.\n","\t\ty1 = cy1 - h1/2.\n","\t\tx1 = int(round(x1))\n","\t\ty1 = int(round(y1))\n","\t\tw1 = int(round(w1))\n","\t\th1 = int(round(h1))\n","\n","\t\treturn x1, y1, w1, h1\n","\n","\texcept ValueError:\n","\t\treturn x, y, w, h\n","\texcept OverflowError:\n","\t\treturn x, y, w, h\n","\texcept Exception as e:\n","\t\tprint(e)\n","\t\treturn x, y, w, h"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-eIDmMML88Q"},"source":["def rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n","\t\"\"\"Convert rpn layer to roi bboxes\n","\n","\tArgs: (num_anchors = 9)\n","\t\trpn_layer: output layer for rpn classification \n","\t\t\tshape (1, feature_map.height, feature_map.width, num_anchors)\n","\t\t\tMight be (1, 18, 25, 9) if resized image is 400 width and 300\n","\t\tregr_layer: output layer for rpn regression\n","\t\t\tshape (1, feature_map.height, feature_map.width, num_anchors)\n","\t\t\tMight be (1, 18, 25, 36) if resized image is 400 width and 300\n","\t\tC: config\n","\t\tuse_regr: Wether to use bboxes regression in rpn\n","\t\tmax_boxes: max bboxes number for non-max-suppression (NMS)\n","\t\toverlap_thresh: If iou in NMS is larger than this threshold, drop the box\n","\n","\tReturns:\n","\t\tresult: boxes from non-max-suppression (shape=(300, 4))\n","\t\t\tboxes: coordinates for bboxes (on the feature map)\n","\t\"\"\"\n","\tregr_layer = regr_layer / C.std_scaling\n","\n","\tanchor_sizes = C.anchor_box_scales   # (3 in here)\n","\tanchor_ratios = C.anchor_box_ratios  # (3 in here)\n","\n","\tassert rpn_layer.shape[0] == 1\n","\n","\t(rows, cols) = rpn_layer.shape[1:3]\n","\n","\tcurr_layer = 0\n","\n","\t# A.shape = (4, feature_map.height, feature_map.width, num_anchors) \n","\t# Might be (4, 18, 25, 9) if resized image is 400 width and 300\n","\t# A is the coordinates for 9 anchors for every point in the feature map \n","\t# => all 18x25x9=4050 anchors cooridnates\n","\tA = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n","\n","\tfor anchor_size in anchor_sizes:\n","\t\tfor anchor_ratio in anchor_ratios:\n","\t\t\t# anchor_x = (128 * 1) / 16 = 8  => width of current anchor\n","\t\t\t# anchor_y = (128 * 2) / 16 = 16 => height of current anchor\n","\t\t\tanchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n","\t\t\tanchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n","\t\t\t\n","\t\t\t# curr_layer: 0~8 (9 anchors)\n","\t\t\t# the Kth anchor of all position in the feature map (9th in total)\n","\t\t\tregr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4] # shape => (18, 25, 4)\n","\t\t\tregr = np.transpose(regr, (2, 0, 1)) # shape => (4, 18, 25)\n","\n","\t\t\t# Create 18x25 mesh grid\n","\t\t\t# For every point in x, there are all the y points and vice versa\n","\t\t\t# X.shape = (18, 25)\n","\t\t\t# Y.shape = (18, 25)\n","\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n","\n","\t\t\t# Calculate anchor position and size for each feature map point\n","\t\t\tA[0, :, :, curr_layer] = X - anchor_x/2 # Top left x coordinate\n","\t\t\tA[1, :, :, curr_layer] = Y - anchor_y/2 # Top left y coordinate\n","\t\t\tA[2, :, :, curr_layer] = anchor_x       # width of current anchor\n","\t\t\tA[3, :, :, curr_layer] = anchor_y       # height of current anchor\n","\n","\t\t\t# Apply regression to x, y, w and h if there is rpn regression layer\n","\t\t\tif use_regr:\n","\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n","\n","\t\t\t# Avoid width and height exceeding 1\n","\t\t\tA[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n","\t\t\tA[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n","\n","\t\t\t# Convert (x, y , w, h) to (x1, y1, x2, y2)\n","\t\t\t# x1, y1 is top left coordinate\n","\t\t\t# x2, y2 is bottom right coordinate\n","\t\t\tA[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n","\t\t\tA[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n","\n","\t\t\t# Avoid bboxes drawn outside the feature map\n","\t\t\tA[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n","\t\t\tA[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n","\t\t\tA[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n","\t\t\tA[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n","\n","\t\t\tcurr_layer += 1\n","\n","\tall_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape=(4050, 4)\n","\tall_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))                   # shape=(4050,)\n","\n","\tx1 = all_boxes[:, 0]\n","\ty1 = all_boxes[:, 1]\n","\tx2 = all_boxes[:, 2]\n","\ty2 = all_boxes[:, 3]\n","\n","\t# Find out the bboxes which is illegal and delete them from bboxes list\n","\tidxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n","\n","\tall_boxes = np.delete(all_boxes, idxs, 0)\n","\tall_probs = np.delete(all_probs, idxs, 0)\n","\n","\t# Apply non_max_suppression\n","\t# Only extract the bboxes. Don't need rpn probs in the later process\n","\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n","\n","\treturn result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"poH9LB_yMZNb"},"source":["#### Format image"]},{"cell_type":"code","metadata":{"id":"YDth5tVJMa7K"},"source":["def format_img_size(img, C):\n","\t\"\"\" formats the image size based on config \"\"\"\n","\timg_min_side = float(C.im_size)\n","\t(height,width,_) = img.shape\n","\t\t\n","\tif width <= height:\n","\t\tratio = img_min_side/width\n","\t\tnew_height = int(ratio * height)\n","\t\tnew_width = int(img_min_side)\n","\telse:\n","\t\tratio = img_min_side/height\n","\t\tnew_width = int(ratio * width)\n","\t\tnew_height = int(img_min_side)\n","\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n","\treturn img, ratio\t\n","\n","def format_img_channels(img, C):\n","\t\"\"\" formats the image channels based on config \"\"\"\n","\timg = img[:, :, (2, 1, 0)]\n","\timg = img.astype(np.float32)\n","\timg[:, :, 0] -= C.img_channel_mean[0]\n","\timg[:, :, 1] -= C.img_channel_mean[1]\n","\timg[:, :, 2] -= C.img_channel_mean[2]\n","\timg /= C.img_scaling_factor\n","\timg = np.transpose(img, (2, 0, 1))\n","\timg = np.expand_dims(img, axis=0)\n","\treturn img\n","\n","def format_img(img, C):\n","\t\"\"\" formats an image for model prediction based on config \"\"\"\n","\timg, ratio = format_img_size(img, C)\n","\timg = format_img_channels(img, C)\n","\treturn img, ratio\n","\n","# Method to transform the coordinates of the bounding box to its original size\n","def get_real_coordinates(ratio, x1, y1, x2, y2):\n","\n","\treal_x1 = int(round(x1 // ratio))\n","\treal_y1 = int(round(y1 // ratio))\n","\treal_x2 = int(round(x2 // ratio))\n","\treal_y2 = int(round(y2 // ratio))\n","\n","\treturn (real_x1, real_y1, real_x2 ,real_y2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ext8BWMMFZO"},"source":["### Load model"]},{"cell_type":"code","metadata":{"id":"HVyedbSHMJib"},"source":["# Main folder\n","base_path = '/content/drive/My Drive/Colab Notebooks/RCNN/Public'\n","\n","test_path = os.path.join(base_path, 'data/test_annotation.txt') # Test data (annotation file)\n","\n","test_base_path = 'BIT-Vehicle/BITVehicle_Dataset' # Directory to save the test images\n","\n","config_output_filename = os.path.join(base_path, 'model_vgg_config.pickle')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"plTgcbQyMRPz"},"source":["with open(config_output_filename, 'rb') as f_in:\n","\tC = pickle.load(f_in)\n","\n","# turn off any data augmentation at test time\n","C.use_horizontal_flips = False\n","C.use_vertical_flips = False\n","C.rot_90 = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZojZhgXPMSlt"},"source":["# Load the records\n","record_df = pd.read_csv(C.record_path)\n","\n","r_epochs = len(record_df)\n","\n","plt.figure(figsize=(15,5))\n","plt.subplot(1,2,1)\n","plt.plot(np.arange(0, r_epochs), record_df['mean_overlapping_bboxes'], 'r')\n","plt.title('mean_overlapping_bboxes')\n","\n","plt.subplot(1,2,2)\n","plt.plot(np.arange(0, r_epochs), record_df['class_acc'], 'r')\n","plt.title('class_acc')\n","\n","plt.show()\n","\n","plt.figure(figsize=(15,5))\n","\n","plt.subplot(1,2,1)\n","plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_cls'], 'r')\n","plt.title('loss_rpn_cls')\n","\n","plt.subplot(1,2,2)\n","plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_regr'], 'r')\n","plt.title('loss_rpn_regr')\n","plt.show()\n","plt.figure(figsize=(15,5))\n","plt.subplot(1,2,1)\n","plt.plot(np.arange(0, r_epochs), record_df['loss_class_cls'], 'r')\n","plt.title('loss_class_cls')\n","\n","plt.subplot(1,2,2)\n","plt.plot(np.arange(0, r_epochs), record_df['loss_class_regr'], 'r')\n","plt.title('loss_class_regr')\n","plt.show()\n","plt.figure(figsize=(15,5))\n","plt.subplot(1,2,1)\n","plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'r')\n","plt.title('total_loss')\n","\n","plt.subplot(1,2,2)\n","plt.plot(np.arange(0, r_epochs), record_df['elapsed_time'], 'r')\n","plt.title('elapsed_time')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHdaU3zdMfmo"},"source":["num_features = 512\n","\n","input_shape_img = (None, None, 3)\n","input_shape_features = (None, None, num_features)\n","\n","img_input = Input(shape=input_shape_img)\n","roi_input = Input(shape=(C.num_rois, 4))\n","feature_map_input = Input(shape=input_shape_features)\n","\n","# define the base network (VGG here, can be Resnet50, Inception, etc)\n","shared_layers = nn_base(img_input, trainable=True)\n","\n","# define the RPN, built on the base layers\n","num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n","rpn_layers = rpn_layer(shared_layers, num_anchors)\n","\n","classifier = classifier_layer(feature_map_input, roi_input, C.num_rois, nb_classes=len(C.class_mapping))\n","model_rpn = Model(img_input, rpn_layers)\n","model_classifier_only = Model([feature_map_input, roi_input], classifier)\n","\n","model_classifier = Model([feature_map_input, roi_input], classifier)\n","\n","print('Loading weights from {}'.format(C.model_path))\n","model_rpn.load_weights(C.model_path, by_name=True)\n","model_classifier.load_weights(C.model_path, by_name=True)\n","\n","model_rpn.compile(optimizer='sgd', loss='mse')\n","model_classifier.compile(optimizer='sgd', loss='mse')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aXObO72p7Jhw"},"source":["## Search"]},{"cell_type":"code","metadata":{"id":"KQyfm9DPlbw9"},"source":["import json\n","# Load file\n","save_train_imgs_loc = os.path.join(base_path, 'data/train_imgs.txt')\n","save_classes_count_loc = os.path.join(base_path, 'data/classes_count.txt')\n","save_class_mapping_loc = os.path.join(base_path, 'data/class_mapping.txt')\n","with open(save_train_imgs_loc) as f1:\n","  train_imgs = json.load(f1)\n","with open(save_classes_count_loc) as f2:\n","  classes_count = json.load(f2)\n","with open(save_class_mapping_loc) as f3:\n","  class_mapping = json.load(f3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8vlDCMuHINJS"},"source":["# Load svm model\r\n","from sklearn.externals import joblib\r\n","final_model = joblib.load(os.path.join(base_path,\"model/SVM_SGD.pkl\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QXTIMPdGIRuM"},"source":["#@title crop_feature3(img_name, test_base_path=None, vehicles=None, bbox_threshold = 0.7, verbose=True) {display-mode: \"form\"}\r\n","out_fea3 = model_classifier_only.get_layer(model_classifier_only.layers[-4].name).output\r\n","out_reg3 = model_classifier_only.get_layer(model_classifier_only.layers[-1].name).output\r\n","model3 = Model(model_classifier_only.input, out_fea3)\r\n","def crop_feature3(img_name, test_base_path=None, vehicles=None, bbox_threshold = 0.7, verbose=True):\r\n","  ''' The function predict the label and apply the bbox of the vehicles.\r\n","\r\n","  Args:\r\n","    imgs_path: List/array of the test img.\r\n","    test_base_path: Folder contain imgs. (Default=None)\r\n","    vehicles: List/array of the labels and bboxes each img. (Default=None)\r\n","    bbox_threshold: If the box classification value is less than this,\r\n","    we ignore this box. (Default = 0.7)\r\n","\r\n","  '''\r\n","  # Check is image file.\r\n","  if not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):\r\n","    return \"Wrong format\"\r\n","\r\n","  id_bg = np.where(final_model.classes_ == 'bg')[0][0]\r\n","  st = time.time() # Start count time to predict\r\n","  filepath = img_name\r\n","  if test_base_path is not None:\r\n","    filepath = os.path.join(test_base_path, img_name)\r\n","  # Read the img\r\n","  img = cv2.imread(filepath) # img.shape=(1200, 1600, 3)\r\n","\r\n","  # Resize img to input model size.\r\n","  # Return img resized and the ratio resized.\r\n","  # e.g: img(1200, 1600) => img_resized(600,800), ratio = 0.5\r\n","  X, ratio = format_img(img, C) # X.shape=(1, 3, 600, 800), ratio=0.5\r\n","  # Format the img\r\n","  X = np.transpose(X, (0, 2, 3, 1)) # X.shape=(1, 600, 800, 3)\r\n","\r\n","  # get output layer Y1, Y2 from the RPN and the feature maps F\r\n","  # Y1: y_rpn_cls\r\n","  # Y2: y_rpn_regr\r\n","  # Y1.shape = (1, 37, 50, 9)\r\n","  # Y2.shape = (1, 37, 50, 36)\r\n","  # F.shape = (1, 37, 50, 512)\r\n","  [Y1, Y2, F] = model_rpn.predict(X)\r\n","\r\n","  # Get bboxes by applying NMS \r\n","  # R.shape = (300, 4)\r\n","  # 4 = (x1,y1,x2,y2)\r\n","  # It mean R contain 300 couple of points (x1,y1,x2,y2)\r\n","  R = rpn_to_roi(Y1, Y2, C, K.common.image_dim_ordering(), overlap_thresh=0.7)\r\n","\r\n","  # convert from (x1,y1,x2,y2) to (x,y,w,h)\r\n","  R[:, 2] -= R[:, 0]\r\n","  R[:, 3] -= R[:, 1]\r\n","\r\n","  # apply the spatial pyramid pooling to the proposed regions\r\n","  # bboxes of objects in the img.\r\n","  bboxes = {} # e.g: {'Sedan': [0.98768216, 0.7442094, ..., 0.9753626, 0.9117886, 0.9728151]}\r\n","  # probability of the object class in bboxes\r\n","  probs = {} # e.g: {'Sedan': [[320, 144, 608, 432], [336, 144, 640, 432], ..., [336, 144, 640, 448], [320, 144, 608, 416], [336, 144, 640, 432]]}\r\n","  # len(bboxes) = len(probs)\r\n","  features = {}\r\n","\r\n","  # Predict bboxes and classname\r\n","  for jk in range(R.shape[0]//C.num_rois + 1):\r\n","      ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\r\n","      if ROIs.shape[1] == 0:\r\n","          break\r\n","      if jk == R.shape[0]//C.num_rois:\r\n","        #pad R\r\n","        curr_shape = ROIs.shape\r\n","        target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\r\n","        ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\r\n","        ROIs_padded[:, :curr_shape[1], :] = ROIs\r\n","        ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\r\n","        ROIs = ROIs_padded\r\n","\r\n","      # [P_cls2, P_features2] = model2.predict([F, ROIs])\r\n","      P_features= model3.predict([F, ROIs])\r\n","      P_features = P_features.reshape((P_features.shape[0]*P_features.shape[1],4096))\r\n","      # print(P_features.shape)\r\n","      P_cls = final_model.predict_proba(P_features)\r\n","      # P_cls3 = final_model.predict_proba(P_features2.reshape((P_features2.shape[0]*P_features2.shape[1],4096)))\r\n","\r\n","      # bb = [final_model.classes_[np.argmax(tt)] for tt in P_cls]\r\n","      # bb2 = [class_mapping[np.argmax(tt2)] for tt2 in P_cls2[0]]\r\n","      # bb3 = [final_model.classes_[np.argmax(tt3)] for tt3 in P_cls3]\r\n","      # print(bb)\r\n","      # print(bb2)\r\n","      # print(bb3)\r\n","      # print(P_cls.shape)\r\n","      # print(P_features.shape)\r\n","\r\n","      # Calculate bboxes coordinates on resized image\r\n","      for ii in range(P_cls.shape[0]):\r\n","        # Ignore 'bg' class\r\n","        if np.max(P_cls[ii, :]) < bbox_threshold or np.argmax(P_cls[ii, :]) == id_bg:\r\n","            continue\r\n","\r\n","        # cls_name = class_mapping[np.argmax(P_cls[ii, :])]\r\n","        cls_name = final_model.classes_[np.argmax(P_cls[ii, :])]\r\n","        # Add class if not exist in bboxes\r\n","        if cls_name not in bboxes:\r\n","            bboxes[cls_name] = []\r\n","            probs[cls_name] = []\r\n","            features[cls_name] = []\r\n","\r\n","        probs[cls_name].append(np.max(P_cls[ii, :]))\r\n","        features[cls_name].append(P_features[ii])\r\n","\r\n","  # all_dets contain the results.\r\n","  # [('Sedan', 99.34011101722717)]\r\n","  all_dets = []\r\n","\r\n","\r\n","  for key in probs:\r\n","      prob = np.array(probs[key])\r\n","      for bb in range(prob.shape[0]):\r\n","        new_probs = prob[bb]\r\n","        feat = features[key][bb]\r\n","        all_dets.append((100*new_probs,feat))\r\n","      \r\n","\r\n","  if verbose == True:\r\n","    print('Elapsed time = {}'.format(time.time() - st))\r\n","  return all_dets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E0gilrqw23Z0"},"source":["### Create image feature database"]},{"cell_type":"code","metadata":{"id":"yI-GzhWjIdqv"},"source":["featureDB = df_train\r\n","featureDB['isExtracted'] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7X6wnZZPrCqT"},"source":["# ''' This cell for extract img feature to save on disk\n","# '''\n","# f = 4096\n","# t = AnnoyIndex(f,'euclidean')\n","\n","# class_mapping = {v: k for k, v in C.class_mapping.items() if k!='bg'}\n","# total_sample = len(featureDB)\n","# for i in range(total_sample):\n","#   sys.stdout.write('\\r'+f\"idx={i}/{total_sample}\")\n","#   tmp_img = featureDB.iloc[i][\"name\"]\n","#   X = crop_feature3(tmp_img, test_base_path=test_base_path,verbose=False)\n","#   if len(X) < 1:\n","#     featureDB.loc[i,['isExtracted']] = 0\n","#     continue\n","#   tmp_results = X[np.argmax([X[i][0] for i in range(len(X))])] # Get feature with the highest prob\n","#   feature = tmp_results[1]\n","#   feature = feature.flatten()\n","#   t.add_item(i, feature)\n","\n","# t.build(1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmXadei8zT3I"},"source":["# t.save(os.path.join(base_path, \"Db_featureSVM.ann\"))\r\n","# featureDB.to_csv(os.path.join(base_path, \"Df_featureSVM.csv\"), index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jsTvVS4B2_Ky"},"source":["### Load feature database"]},{"cell_type":"code","metadata":{"id":"v9kQm672n67O"},"source":["# Load extracted feature\n","save_db_path = os.path.join(base_path, \"Db_featureSVM.ann\")\n","f = 4096\n","vehicle_query = AnnoyIndex(f,'euclidean')\n","vehicle_query.load(save_db_path)\n","featureDB = pd.read_csv(os.path.join(base_path, \"Df_featureSVM.csv\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FstCEWHVTc1a"},"source":["# Evaluate"]},{"cell_type":"code","metadata":{"id":"Eb-DnfPAugMr"},"source":["import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","# Annoy uses Euclidean distance of normalized vectors for its angular distance, which for two vectors u,v is equal to sqrt(2-2*cos(u, v))\n","def custom_cosine_similarity(a,b):\n","  a = np.array(a)\n","  b = np.array(b)\n","  tu = sum(a*b)\n","  mau = np.sqrt(sum(a**2))*np.sqrt(sum(b**2))\n","  return tu/mau\n","\n","def euclidean(u, v):\n","  cos = round(cosine_similarity(np.expand_dims(u,axis=0),np.expand_dims(v,axis=0))[0,0], 4)\n","  return np.sqrt(2*(1-cos))\n","\n","def custom_euclidean(u, v):\n","  cos = round(custom_cosine_similarity(u,v), 4)\n","  return np.sqrt(2*(1-cos))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5nBLfogxVWsY"},"source":["# SGDClassifier"]},{"cell_type":"code","metadata":{"id":"oc7E3tYNDxzw"},"source":["db_vector = [vehicle_query.get_item_vector(i) for i in range(vehicle_query.get_n_items())]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1R9hUzTOD8Na"},"source":["db_count = featureDB[featureDB['isExtracted']==1]['label'].value_counts()\r\n","n_items = vehicle_query.get_n_items()\r\n","def eval_feat(feat, disitance_function, label=None, threshold=0.5, top_k=5000):\r\n","  AP = 0.0\r\n","  Relevant_Img = 0\r\n","  T = []\r\n","  P = []\r\n","  n_true = 1 if label is None else db_count[label]\r\n","\r\n","  if len(feat) < 1:\r\n","    if label is None:\r\n","      return 1.0, [1], [1]\r\n","    else:\r\n","      return 0.0, np.ones(shape=(n_true,1)).ravel().tolist(), np.zeros(shape=(n_true,1)).ravel().tolist()\r\n","  feature = feat\r\n","  # feature = feat.flatten()\r\n","  vehicle_query_result = vehicle_query.get_nns_by_vector(feature, n=top_k)\r\n","  selected_result = []\r\n","  distance_arr = []\r\n","  cos_sim = []\r\n","  for i in vehicle_query_result:\r\n","      tmp_cos = cosineS(feature,db_vector[i])\r\n","      tmp_distance = np.sqrt(2*(1-tmp_cos))\r\n","      # if tmp_distance < threshold:\r\n","      selected_result.append(i)\r\n","      distance_arr.append(tmp_distance)\r\n","      cos_sim.append(tmp_cos)\r\n","  index_arr = np.argsort(distance_arr)\r\n","  if len(selected_result) == 0:\r\n","    return 0.0, np.ones(shape=(n_true,1)).ravel().tolist(), np.zeros(shape=(n_true,1)).ravel().tolist()\r\n","\r\n","  for i in range(len(selected_result)):\r\n","    img_index = selected_result[index_arr[i]]\r\n","    img_label = featureDB.loc[img_index]['label']\r\n","    P.append(cos_sim[index_arr[i]])\r\n","    if img_label == label:\r\n","      T.append(1)\r\n","      Relevant_Img += 1\r\n","      AP += (Relevant_Img / (i+1))\r\n","    else:\r\n","      T.append(0)\r\n","  if Relevant_Img==0:\r\n","    return 0, T, P\r\n","  return AP/Relevant_Img, T, P"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qa6RHd8kD96b"},"source":["def eval_feats(img_paths, image_labels, distance_function, threshold=0.5, top_k=5000):\r\n","  APs = {k:[] for k in C.class_mapping if k!='bg'}\r\n","  T = {k:[] for k in C.class_mapping if k!='bg'}\r\n","  P = {k:[] for k in C.class_mapping if k!='bg'}\r\n","  mAPs = []\r\n","  i = 0\r\n","  n_sample = len(image_labels)\r\n","  for img_path, img_label in zip(img_paths,image_labels):\r\n","    i += 1\r\n","    sys.stdout.write('\\r'+'idx=' + str(i) + '/' + str(n_sample))\r\n","    \r\n","    img_feat = []\r\n","    X = crop_feature3(img_path, test_base_path=test_base_path,verbose=False)\r\n","    if X:\r\n","      tmp_results = X[np.argmax([X[i][0] for i in range(len(X))])]\r\n","      img_feat = tmp_results[1]\r\n","    AP, t, p = eval_feat(img_feat, distance_function, label=img_label, threshold=threshold, top_k=top_k)\r\n","    T[img_label] += t\r\n","    P[img_label] += p\r\n","    APs[img_label].append(AP)\r\n","    mAPs.append(average_precision_score(t, p))\r\n","  return APs, T, P, mAPs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P6MYflx9EBr7"},"source":["# Switch key value for class mapping\r\n","class_mapping = C.class_mapping\r\n","class_mapping = {v: k for k, v in class_mapping.items()}\r\n","\r\n","n_items = vehicle_query.get_n_items()\r\n","imgs_test = df_test[df_test['nVehicles']<2]['name']\r\n","labels_test = df_test[df_test['nVehicles']<2]['vehicles'].str['category']\r\n","AP, T, P, mAPs = eval_feats(imgs_test, labels_test, euclidean, threshold=0.7, top_k=n_items)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzIeDGlXEtLw"},"source":["skl_mAP = np.round(np.mean(mAPs),4)\r\n","APs = {k:np.round(np.mean(AP[k]),4) for k in AP}\r\n","total_T, total_P, total_APs = [], [], []\r\n","for k in T:\r\n","  total_T += T[k]\r\n","  total_P += P[k]\r\n","  total_APs += AP[k]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-27MFqH5G9Wn"},"source":["my_mAP = round(np.mean(total_APs), 4)\r\n","my_mAP"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3LeXhxK_E3Zn"},"source":["tmp_P = {k:precision_score(T[k], P[k]) for k in T}\r\n","tmp_P"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Ui1uMIqE6Wl"},"source":["tmp_R = {k:recall_score(T[k], P[k]) for k in T}\r\n","tmp_R"],"execution_count":null,"outputs":[]}]}